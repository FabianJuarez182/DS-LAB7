{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Lab7.ipynb**\n",
    "\n",
    "#### Activity\n",
    "- Data science - Laboratorio 7\n",
    "\n",
    "#### Authors: \n",
    "- Diego Lemus\n",
    "- Fabián Juárez\n",
    "\n",
    "\n",
    "#### Date:\n",
    "- 22 de Septiembre 2024\n",
    "\n",
    "\n",
    "#### Repository:\n",
    "- Link: https://github.com/FabianJuarez182/DS-LAB7.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 y 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\domot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          rawContent  \\\n",
      "0  _\\nConfirmado Compañeres,\\n\\nEl impuesto por l...   \n",
      "1  #URGENTE Lo que los medios #faferos no informa...   \n",
      "2  @IvanDuque @BArevalodeLeon Con que usaste PEGA...   \n",
      "3  @IvanDuque @BArevalodeLeon Entre Ellos se enti...   \n",
      "4  El presidente @BArevalodeLeon y la vicepreside...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  confirmado compañeres impuesto —solo cuenta pa...  \n",
      "1  urgente medios faferos informaron ayer acerca ...  \n",
      "2  usaste pegasus espiar detractores obra narcisi...  \n",
      "3         entienden bien cuadrando productivareunión  \n",
      "4  presidente vicepresidenta participan sesión so...  \n",
      "[('si', 462), ('presidente', 430), ('gobierno', 334), ('solo', 287), ('guatemala', 245), ('vos', 197), ('ser', 185), ('q', 181), ('corruptos', 177), ('bien', 171)]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar las stopwords en español de nltk\n",
    "nltk.download('stopwords')\n",
    "spanish_stopwords = set(stopwords.words('spanish'))\n",
    "\n",
    "# Cargar el archivo .txt con manejo de errores para JSON\n",
    "def load_json_txt(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-16') as file:  # Cambiamos a utf-16\n",
    "        for line in file:\n",
    "            line = line.strip()  # Eliminar espacios en blanco y saltos de línea\n",
    "            if line:  # Verificar que la línea no esté vacía\n",
    "                try:\n",
    "                    data.append(json.loads(line))\n",
    "                except json.JSONDecodeError:\n",
    "                    continue  # Ignorar líneas mal formadas\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Función para limpiar el texto\n",
    "def clean_text(text):\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    # Eliminar URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Eliminar caracteres especiales, menciones y números\n",
    "    text = re.sub(r'\\@\\w+|\\#|\\d+', '', text)\n",
    "    # Eliminar signos de puntuación\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Eliminar stopwords en español\n",
    "    text = ' '.join([word for word in text.split() if word not in spanish_stopwords])\n",
    "    return text\n",
    "\n",
    "# Cargar el archivo de texto\n",
    "df = load_json_txt('tioberny.txt')\n",
    "\n",
    "# Aplicar la función de limpieza a la columna de texto\n",
    "df['cleaned_text'] = df['rawContent'].apply(clean_text)\n",
    "\n",
    "# Visualizar los datos limpiados\n",
    "print(df[['rawContent', 'cleaned_text']].head())\n",
    "\n",
    "# Calcular la frecuencia de las palabras\n",
    "word_freq = Counter(\" \".join(df['cleaned_text']).split())\n",
    "\n",
    "# Mostrar las palabras más frecuentes después de la limpieza\n",
    "print(word_freq.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteo de interacciones por tipo:\n",
      "type\n",
      "mention    7336\n",
      "reply      3330\n",
      "Name: count, dtype: int64\n",
      "Grafo de mention exportado a Gephi como 'user_interactions_graph_mention.gexf'\n",
      "Grafo de reply exportado a Gephi como 'user_interactions_graph_reply.gexf'\n",
      "No se encontraron interacciones de tipo retweet\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Función ajustada para extraer respuestas, menciones y retweets\n",
    "def extract_interactions(row):\n",
    "    interactions = []\n",
    "    \n",
    "    # Validar que 'user' y 'username' existan y sean cadenas\n",
    "    if 'user' in row and isinstance(row['user'], dict) and 'username' in row['user']:\n",
    "        user = row['user']['username'].lower() if isinstance(row['user']['username'], str) else None\n",
    "    else:\n",
    "        user = None\n",
    "\n",
    "    # Extraer respuestas (si existen y son válidas)\n",
    "    if 'inReplyToUser' in row and isinstance(row['inReplyToUser'], dict):\n",
    "        replied_to_user = row['inReplyToUser'].get('username', None)\n",
    "        if user and replied_to_user:\n",
    "            interactions.append(('reply', user, replied_to_user.lower()))\n",
    "    \n",
    "    # Extraer menciones (si existen y son válidas)\n",
    "    if 'mentionedUsers' in row and isinstance(row['mentionedUsers'], list):\n",
    "        for mention in row['mentionedUsers']:\n",
    "            if 'username' in mention and isinstance(mention['username'], str):\n",
    "                interactions.append(('mention', user, mention['username'].lower()))\n",
    "    \n",
    "    # Extraer retweets (si existen y son válidos)\n",
    "    if 'retweetedTweet' in row and isinstance(row['retweetedTweet'], dict):\n",
    "        retweeted_user = row['retweetedTweet'].get('user', {}).get('username')\n",
    "        if user and retweeted_user:\n",
    "            interactions.append(('retweet', user, retweeted_user.lower()))\n",
    "    \n",
    "    return interactions\n",
    "\n",
    "# Cargar los datos\n",
    "df = load_json_txt('traficogt.txt')\n",
    "\n",
    "# Aplicar la extracción de interacciones a cada tweet\n",
    "df['interactions'] = df.apply(extract_interactions, axis=1)\n",
    "\n",
    "# Descomponer las interacciones en un DataFrame\n",
    "interactions = []\n",
    "for interaction_list in df['interactions']:\n",
    "    interactions.extend(interaction_list)\n",
    "\n",
    "# Crear un DataFrame con las interacciones\n",
    "interaction_df = pd.DataFrame(interactions, columns=['type', 'source', 'target'])\n",
    "\n",
    "# Eliminar duplicados\n",
    "interaction_df = interaction_df.drop_duplicates()\n",
    "\n",
    "# Contar el número de menciones, respuestas y retweets\n",
    "interaction_counts = interaction_df['type'].value_counts()\n",
    "\n",
    "# Mostrar cuántas interacciones de cada tipo existen\n",
    "print(\"Conteo de interacciones por tipo:\")\n",
    "print(interaction_counts)\n",
    "\n",
    "# Función para crear y exportar un grafo para cada tipo de interacción\n",
    "def create_and_export_graph(interaction_type, interaction_df):\n",
    "    # Filtrar las interacciones por el tipo deseado (mención, respuesta o retweet)\n",
    "    filtered_df = interaction_df[interaction_df['type'] == interaction_type]\n",
    "    \n",
    "    # Verificar si hay interacciones para este tipo\n",
    "    if filtered_df.empty:\n",
    "        print(f\"No se encontraron interacciones de tipo {interaction_type}\")\n",
    "        return\n",
    "    \n",
    "    # Crear un grafo dirigido\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Añadir las interacciones filtradas al grafo\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        G.add_edge(row['source'], row['target'], interaction_type=row['type'])\n",
    "    \n",
    "    # Exportar el grafo a Gephi en formato GEXF\n",
    "    file_name = f\"user_interactions_graph_{interaction_type}.gexf\"\n",
    "    nx.write_gexf(G, file_name)\n",
    "    print(f\"Grafo de {interaction_type} exportado a Gephi como '{file_name}'\")\n",
    "\n",
    "# Crear y exportar grafos para menciones, respuestas y retweets\n",
    "create_and_export_graph('mention', interaction_df)\n",
    "create_and_export_graph('reply', interaction_df)\n",
    "create_and_export_graph('retweet', interaction_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se evidencia que no existen los retweet de manera explicita por lo que se utilizará la cnatidad de retweets de cada tweet para visualizar que tanto auge tienen y que tan influyente es el usuario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
